{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/lazary/anaconda2/envs/NeuralDecomposition/lib/python3.6/site-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from pytorch_pretrained_bert.modeling import BertConfig, BertModel\n",
    "\n",
    "from allennlp.common.testing import ModelTestCase\n",
    "from allennlp.data.dataset import Batch\n",
    "from allennlp.data.fields import TextField, ListField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.token_indexers.wordpiece_indexer import PretrainedBertIndexer\n",
    "from allennlp.data.tokenizers import WordTokenizer\n",
    "from allennlp.data.tokenizers.word_splitter import BertBasicWordSplitter\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.modules.token_embedders.bert_token_embedder import BertEmbedder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# token_indexer = PretrainedBertIndexer(str(vocab_path))\n",
    "\n",
    "# config_path = self.FIXTURES_ROOT / 'bert' / 'config.json'\n",
    "config = BertConfig(vocab_size_or_config_json_file=30522)\n",
    "bert_model = BertModel(config)\n",
    "token_embedder = BertEmbedder(bert_model)\n",
    "\n",
    "bert_name = 'bert-base-uncased'\n",
    "token_indexer = PretrainedBertIndexer(pretrained_model=bert_name, use_starting_offsets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = WordTokenizer(word_splitter=BertBasicWordSplitter())\n",
    "\n",
    "#            2   3    4   3     5     6   8      9    2   14   12\n",
    "sentence1 = \"the quickest quick brown fox jumped over the lazy dog\"\n",
    "tokens1 = tokenizer.tokenize(sentence1)\n",
    "\n",
    "#            2   3     5     6   8      9    2  15 10 11 14   1\n",
    "sentence2 = \"the quick brown fox jumped over the laziest lazy elmo\"\n",
    "tokens2 = tokenizer.tokenize(sentence2)\n",
    "\n",
    "vocab = Vocabulary()\n",
    "\n",
    "instance1 = Instance({\"tokens\": TextField(tokens1, {\"bert\": token_indexer})})\n",
    "instance2 = Instance({\"tokens\": TextField(tokens2, {\"bert\": token_indexer})})\n",
    "\n",
    "batch = Batch([instance1, instance2])\n",
    "batch.index_instances(vocab)\n",
    "\n",
    "padding_lengths = batch.get_padding_lengths()\n",
    "tensor_dict = batch.as_tensor_dict(padding_lengths)\n",
    "tokens = tensor_dict[\"tokens\"]\n",
    "\n",
    "\n",
    "# Offsets, should get 10 vectors back.\n",
    "bert_vectors = token_embedder(tokens[\"bert\"], offsets=tokens[\"bert-offsets\"])\n",
    "assert list(bert_vectors.shape) == [2, 10, 768]\n",
    "\n",
    "# Now try top_layer_only = True\n",
    "tlo_embedder = BertEmbedder(bert_model, top_layer_only=True)\n",
    "bert_vectors = tlo_embedder(tokens[\"bert\"], offsets=tokens[\"bert-offsets\"])\n",
    "assert list(bert_vectors.shape) == [2, 10, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bert_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.93796647e+00, -2.21370563e-01, -4.27188054e-02, ...,\n",
       "         -2.00201765e-01, -2.62679043e-03,  1.89772642e+00],\n",
       "        [-2.84264302e+00,  2.51756072e-01, -1.14900100e+00, ...,\n",
       "          2.18602911e-01,  2.27466643e-01,  1.60831720e-01],\n",
       "        [-2.28935361e+00,  1.27255058e+00, -9.51204360e-01, ...,\n",
       "          8.86204302e-01,  1.15967286e+00, -5.60903192e-01],\n",
       "        ...,\n",
       "        [-1.89784360e+00, -8.26657474e-01, -1.54118264e+00, ...,\n",
       "         -1.32414654e-01,  2.39476383e-01,  1.58546090e-01],\n",
       "        [-1.79235995e+00,  1.04425693e+00,  1.96723536e-01, ...,\n",
       "          8.53676200e-01,  1.08933592e+00,  1.38736570e+00],\n",
       "        [-9.26102638e-01, -8.25094283e-02, -1.17659962e+00, ...,\n",
       "          2.74015963e-01, -2.32073471e-01, -8.63033310e-02]],\n",
       "\n",
       "       [[-1.51391673e+00,  6.94965363e-01, -1.46800399e+00, ...,\n",
       "         -1.05817728e-01, -3.58425975e-01,  9.83054340e-01],\n",
       "        [-3.24107230e-01,  4.46848214e-01, -1.27278852e+00, ...,\n",
       "         -5.27161777e-01,  1.14167440e+00,  1.61995876e+00],\n",
       "        [-2.51549029e+00,  1.06713690e-01, -1.21050656e+00, ...,\n",
       "          4.36487913e-01, -2.75806248e-01,  6.82874203e-01],\n",
       "        ...,\n",
       "        [-7.64951587e-01, -6.42145872e-01,  5.85604310e-01, ...,\n",
       "          9.84424129e-02, -2.50364184e-01,  2.30315328e+00],\n",
       "        [-1.13948083e+00,  2.38845873e+00, -1.18472934e-01, ...,\n",
       "         -7.94781625e-01,  7.64866412e-01, -3.17352474e-01],\n",
       "        [-4.79742438e-01, -8.20087850e-01, -2.83169121e-01, ...,\n",
       "         -5.31234801e-01,  4.13090527e-01,  1.87972617e+00]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_vectors.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "allennlp.data.tokenizers.token.Token"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens1[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuralDecomposition",
   "language": "python",
   "name": "neuraldecomposition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
